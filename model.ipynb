{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting torch==2.1.0\n",
      "  Downloading torch-2.1.0-cp311-cp311-win_amd64.whl.metadata (25 kB)\n",
      "Collecting torchtext==0.16.0\n",
      "  Downloading torchtext-0.16.0-cp311-cp311-win_amd64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from torch==2.1.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from torch==2.1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from torch==2.1.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from torch==2.1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from torch==2.1.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from torch==2.1.0) (2025.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from torchtext==0.16.0) (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from torchtext==0.16.0) (2.32.3)\n",
      "Collecting torchdata==0.7.0 (from torchtext==0.16.0)\n",
      "  Downloading torchdata-0.7.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from torchdata==0.7.0->torchtext==0.16.0) (2.3.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch==2.1.0)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from requests->torchtext==0.16.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from requests->torchtext==0.16.0) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from requests->torchtext==0.16.0) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from tqdm->torchtext==0.16.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from jinja2->torch==2.1.0) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from sympy->torch==2.1.0) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hasan\\downloads\\alanbrb\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading torch-2.1.0-cp311-cp311-win_amd64.whl (192.3 MB)\n",
      "   ---------------------------------------- 0.0/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/192.3 MB 223.7 kB/s eta 0:14:18\n",
      "   ---------------------------------------- 0.5/192.3 MB 223.7 kB/s eta 0:14:18\n",
      "   ---------------------------------------- 0.5/192.3 MB 223.7 kB/s eta 0:14:18\n",
      "   ---------------------------------------- 0.5/192.3 MB 223.7 kB/s eta 0:14:18\n",
      "   ---------------------------------------- 0.5/192.3 MB 223.7 kB/s eta 0:14:18\n",
      "   ---------------------------------------- 0.5/192.3 MB 223.7 kB/s eta 0:14:18\n",
      "   ---------------------------------------- 0.5/192.3 MB 223.7 kB/s eta 0:14:18\n",
      "   ---------------------------------------- 0.5/192.3 MB 223.7 kB/s eta 0:14:18\n",
      "   ---------------------------------------- 0.5/192.3 MB 223.7 kB/s eta 0:14:18\n",
      "   ---------------------------------------- 0.5/192.3 MB 223.7 kB/s eta 0:14:18\n",
      "   ---------------------------------------- 0.5/192.3 MB 223.7 kB/s eta 0:14:18\n",
      "   ---------------------------------------- 0.5/192.3 MB 223.7 kB/s eta 0:14:18\n",
      "   ---------------------------------------- 0.5/192.3 MB 223.7 kB/s eta 0:14:18\n",
      "   ---------------------------------------- 0.5/192.3 MB 223.7 kB/s eta 0:14:18\n",
      "   ---------------------------------------- 0.5/192.3 MB 223.7 kB/s eta 0:14:18\n",
      "   ---------------------------------------- 0.8/192.3 MB 118.6 kB/s eta 0:26:56\n",
      "   ---------------------------------------- 0.8/192.3 MB 118.6 kB/s eta 0:26:56\n",
      "   ---------------------------------------- 0.8/192.3 MB 118.6 kB/s eta 0:26:56\n",
      "   ---------------------------------------- 0.8/192.3 MB 118.6 kB/s eta 0:26:56\n",
      "   ---------------------------------------- 0.8/192.3 MB 118.6 kB/s eta 0:26:56\n",
      "   ---------------------------------------- 0.8/192.3 MB 118.6 kB/s eta 0:26:56\n",
      "   ---------------------------------------- 0.8/192.3 MB 118.6 kB/s eta 0:26:56\n",
      "   ---------------------------------------- 0.8/192.3 MB 118.6 kB/s eta 0:26:56\n",
      "   ---------------------------------------- 0.8/192.3 MB 118.6 kB/s eta 0:26:56\n",
      "   ---------------------------------------- 0.8/192.3 MB 118.6 kB/s eta 0:26:56\n",
      "   ---------------------------------------- 0.8/192.3 MB 118.6 kB/s eta 0:26:56\n",
      "   ---------------------------------------- 0.8/192.3 MB 118.6 kB/s eta 0:26:56\n",
      "   ---------------------------------------- 0.8/192.3 MB 118.6 kB/s eta 0:26:56\n",
      "   ---------------------------------------- 1.0/192.3 MB 109.4 kB/s eta 0:29:09\n",
      "   ---------------------------------------- 1.0/192.3 MB 109.4 kB/s eta 0:29:09\n",
      "   ---------------------------------------- 1.0/192.3 MB 109.4 kB/s eta 0:29:09\n",
      "   ---------------------------------------- 1.0/192.3 MB 109.4 kB/s eta 0:29:09\n",
      "   ---------------------------------------- 1.0/192.3 MB 109.4 kB/s eta 0:29:09\n",
      "   ---------------------------------------- 1.0/192.3 MB 109.4 kB/s eta 0:29:09\n",
      "   ---------------------------------------- 1.0/192.3 MB 109.4 kB/s eta 0:29:09\n",
      "   ---------------------------------------- 1.0/192.3 MB 109.4 kB/s eta 0:29:09\n",
      "   ---------------------------------------- 1.0/192.3 MB 109.4 kB/s eta 0:29:09\n",
      "   ---------------------------------------- 1.0/192.3 MB 109.4 kB/s eta 0:29:09\n",
      "   ---------------------------------------- 1.0/192.3 MB 109.4 kB/s eta 0:29:09\n",
      "   ---------------------------------------- 1.0/192.3 MB 109.4 kB/s eta 0:29:09\n",
      "   ---------------------------------------- 1.0/192.3 MB 109.4 kB/s eta 0:29:09\n",
      "   ---------------------------------------- 1.0/192.3 MB 109.4 kB/s eta 0:29:09\n",
      "   ---------------------------------------- 1.0/192.3 MB 109.4 kB/s eta 0:29:09\n",
      "   ---------------------------------------- 1.0/192.3 MB 109.4 kB/s eta 0:29:09\n",
      "   ---------------------------------------- 1.3/192.3 MB 99.3 kB/s eta 0:32:05\n",
      "   ---------------------------------------- 1.3/192.3 MB 99.3 kB/s eta 0:32:05\n",
      "   ---------------------------------------- 1.3/192.3 MB 99.3 kB/s eta 0:32:05\n",
      "   ---------------------------------------- 1.3/192.3 MB 99.3 kB/s eta 0:32:05\n",
      "   ---------------------------------------- 1.3/192.3 MB 99.3 kB/s eta 0:32:05\n",
      "   ---------------------------------------- 1.3/192.3 MB 99.3 kB/s eta 0:32:05\n",
      "   ---------------------------------------- 1.3/192.3 MB 99.3 kB/s eta 0:32:05\n",
      "   ---------------------------------------- 1.3/192.3 MB 99.3 kB/s eta 0:32:05\n",
      "   ---------------------------------------- 1.3/192.3 MB 99.3 kB/s eta 0:32:05\n",
      "   ---------------------------------------- 1.6/192.3 MB 105.0 kB/s eta 0:30:18\n",
      "   ---------------------------------------- 1.6/192.3 MB 105.0 kB/s eta 0:30:18\n",
      "   ---------------------------------------- 1.6/192.3 MB 105.0 kB/s eta 0:30:18\n",
      "   ---------------------------------------- 1.6/192.3 MB 105.0 kB/s eta 0:30:18\n",
      "   ---------------------------------------- 1.6/192.3 MB 105.0 kB/s eta 0:30:18\n",
      "   ---------------------------------------- 1.6/192.3 MB 105.0 kB/s eta 0:30:18\n",
      "   ---------------------------------------- 1.8/192.3 MB 114.4 kB/s eta 0:27:46\n",
      "   ---------------------------------------- 1.8/192.3 MB 114.4 kB/s eta 0:27:46\n",
      "   ---------------------------------------- 1.8/192.3 MB 114.4 kB/s eta 0:27:46\n",
      "   ---------------------------------------- 1.8/192.3 MB 114.4 kB/s eta 0:27:46\n",
      "   ---------------------------------------- 1.8/192.3 MB 114.4 kB/s eta 0:27:46\n",
      "   ---------------------------------------- 1.8/192.3 MB 114.4 kB/s eta 0:27:46\n",
      "   ---------------------------------------- 1.8/192.3 MB 114.4 kB/s eta 0:27:46\n",
      "   ---------------------------------------- 1.8/192.3 MB 114.4 kB/s eta 0:27:46\n",
      "   ---------------------------------------- 1.8/192.3 MB 114.4 kB/s eta 0:27:46\n",
      "   ---------------------------------------- 1.8/192.3 MB 114.4 kB/s eta 0:27:46\n",
      "   ---------------------------------------- 1.8/192.3 MB 114.4 kB/s eta 0:27:46\n",
      "   ---------------------------------------- 2.1/192.3 MB 113.6 kB/s eta 0:27:56\n",
      "   ---------------------------------------- 2.1/192.3 MB 113.6 kB/s eta 0:27:56\n",
      "   ---------------------------------------- 2.1/192.3 MB 113.6 kB/s eta 0:27:56\n",
      "   ---------------------------------------- 2.1/192.3 MB 113.6 kB/s eta 0:27:56\n",
      "   ---------------------------------------- 2.1/192.3 MB 113.6 kB/s eta 0:27:56\n",
      "   ---------------------------------------- 2.1/192.3 MB 113.6 kB/s eta 0:27:56\n",
      "   ---------------------------------------- 2.4/192.3 MB 119.3 kB/s eta 0:26:33\n",
      "   ---------------------------------------- 2.4/192.3 MB 119.3 kB/s eta 0:26:33\n",
      "   ---------------------------------------- 2.4/192.3 MB 119.3 kB/s eta 0:26:33\n",
      "   ---------------------------------------- 2.4/192.3 MB 119.3 kB/s eta 0:26:33\n",
      "    --------------------------------------- 2.6/192.3 MB 128.6 kB/s eta 0:24:36\n",
      "    --------------------------------------- 2.6/192.3 MB 128.6 kB/s eta 0:24:36\n",
      "    --------------------------------------- 2.6/192.3 MB 128.6 kB/s eta 0:24:36\n",
      "    --------------------------------------- 2.6/192.3 MB 128.6 kB/s eta 0:24:36\n",
      "    --------------------------------------- 2.6/192.3 MB 128.6 kB/s eta 0:24:36\n",
      "    --------------------------------------- 2.6/192.3 MB 128.6 kB/s eta 0:24:36\n",
      "    --------------------------------------- 2.9/192.3 MB 133.8 kB/s eta 0:23:37\n",
      "    --------------------------------------- 2.9/192.3 MB 133.8 kB/s eta 0:23:37\n",
      "    --------------------------------------- 2.9/192.3 MB 133.8 kB/s eta 0:23:37\n",
      "    --------------------------------------- 3.1/192.3 MB 142.6 kB/s eta 0:22:07\n",
      "    --------------------------------------- 3.1/192.3 MB 142.6 kB/s eta 0:22:07\n",
      "    --------------------------------------- 3.4/192.3 MB 152.2 kB/s eta 0:20:42\n",
      "    --------------------------------------- 3.4/192.3 MB 152.2 kB/s eta 0:20:42\n",
      "    --------------------------------------- 3.7/192.3 MB 162.0 kB/s eta 0:19:25\n",
      "    --------------------------------------- 3.9/192.3 MB 171.9 kB/s eta 0:18:16\n",
      "    --------------------------------------- 3.9/192.3 MB 171.9 kB/s eta 0:18:16\n",
      "    --------------------------------------- 3.9/192.3 MB 171.9 kB/s eta 0:18:16\n",
      "    --------------------------------------- 4.2/192.3 MB 179.5 kB/s eta 0:17:29\n",
      "    --------------------------------------- 4.2/192.3 MB 179.5 kB/s eta 0:17:29\n",
      "    --------------------------------------- 4.2/192.3 MB 179.5 kB/s eta 0:17:29\n",
      "    --------------------------------------- 4.2/192.3 MB 179.5 kB/s eta 0:17:29\n",
      "    --------------------------------------- 4.5/192.3 MB 184.2 kB/s eta 0:17:00\n",
      "    --------------------------------------- 4.5/192.3 MB 184.2 kB/s eta 0:17:00\n",
      "    --------------------------------------- 4.7/192.3 MB 191.4 kB/s eta 0:16:21\n",
      "    --------------------------------------- 4.7/192.3 MB 191.4 kB/s eta 0:16:21\n",
      "   - -------------------------------------- 5.0/192.3 MB 199.1 kB/s eta 0:15:42\n",
      "   - -------------------------------------- 5.0/192.3 MB 199.1 kB/s eta 0:15:42\n",
      "   - -------------------------------------- 5.2/192.3 MB 207.3 kB/s eta 0:15:03\n",
      "   - -------------------------------------- 5.5/192.3 MB 215.5 kB/s eta 0:14:27\n",
      "   - -------------------------------------- 5.5/192.3 MB 215.5 kB/s eta 0:14:27\n",
      "   - -------------------------------------- 5.8/192.3 MB 223.8 kB/s eta 0:13:54\n",
      "   - -------------------------------------- 6.0/192.3 MB 232.1 kB/s eta 0:13:23\n",
      "   - -------------------------------------- 6.3/192.3 MB 240.4 kB/s eta 0:12:54\n",
      "   - -------------------------------------- 6.6/192.3 MB 248.7 kB/s eta 0:12:27\n",
      "   - -------------------------------------- 6.8/192.3 MB 257.0 kB/s eta 0:12:02\n",
      "   - -------------------------------------- 7.1/192.3 MB 265.2 kB/s eta 0:11:39\n",
      "   - -------------------------------------- 7.3/192.3 MB 273.4 kB/s eta 0:11:17\n",
      "   - -------------------------------------- 7.6/192.3 MB 280.5 kB/s eta 0:10:59\n",
      "   - -------------------------------------- 7.6/192.3 MB 280.5 kB/s eta 0:10:59\n",
      "   - -------------------------------------- 7.9/192.3 MB 285.9 kB/s eta 0:10:46\n",
      "   - -------------------------------------- 7.9/192.3 MB 285.9 kB/s eta 0:10:46\n",
      "   - -------------------------------------- 7.9/192.3 MB 285.9 kB/s eta 0:10:46\n",
      "   - -------------------------------------- 8.1/192.3 MB 287.9 kB/s eta 0:10:40\n",
      "   - -------------------------------------- 8.1/192.3 MB 287.9 kB/s eta 0:10:40\n",
      "   - -------------------------------------- 8.1/192.3 MB 287.9 kB/s eta 0:10:40\n",
      "   - -------------------------------------- 8.1/192.3 MB 287.9 kB/s eta 0:10:40\n",
      "   - -------------------------------------- 8.4/192.3 MB 288.3 kB/s eta 0:10:39\n",
      "   - -------------------------------------- 8.4/192.3 MB 288.3 kB/s eta 0:10:39\n",
      "   - -------------------------------------- 8.4/192.3 MB 288.3 kB/s eta 0:10:39\n",
      "   - -------------------------------------- 8.4/192.3 MB 288.3 kB/s eta 0:10:39\n",
      "   - -------------------------------------- 8.7/192.3 MB 289.9 kB/s eta 0:10:34\n",
      "   - -------------------------------------- 8.7/192.3 MB 289.9 kB/s eta 0:10:34\n",
      "   - -------------------------------------- 8.7/192.3 MB 289.9 kB/s eta 0:10:34\n",
      "   - -------------------------------------- 8.7/192.3 MB 289.9 kB/s eta 0:10:34\n",
      "   - -------------------------------------- 8.9/192.3 MB 290.0 kB/s eta 0:10:33\n",
      "   - -------------------------------------- 8.9/192.3 MB 290.0 kB/s eta 0:10:33\n",
      "   - -------------------------------------- 9.2/192.3 MB 296.7 kB/s eta 0:10:18\n",
      "   - -------------------------------------- 9.2/192.3 MB 296.7 kB/s eta 0:10:18\n",
      "   - -------------------------------------- 9.4/192.3 MB 301.2 kB/s eta 0:10:08\n",
      "   - -------------------------------------- 9.4/192.3 MB 301.2 kB/s eta 0:10:08\n",
      "   -- ------------------------------------- 9.7/192.3 MB 306.5 kB/s eta 0:09:56\n",
      "   -- ------------------------------------ 10.0/192.3 MB 340.2 kB/s eta 0:08:57\n",
      "   -- ------------------------------------ 10.0/192.3 MB 340.2 kB/s eta 0:08:57\n",
      "   -- ------------------------------------ 10.2/192.3 MB 346.5 kB/s eta 0:08:46\n",
      "   -- ------------------------------------ 10.5/192.3 MB 352.9 kB/s eta 0:08:36\n",
      "   -- ------------------------------------ 10.7/192.3 MB 359.4 kB/s eta 0:08:26\n",
      "   -- ------------------------------------ 11.0/192.3 MB 365.9 kB/s eta 0:08:16\n",
      "   -- ------------------------------------ 11.3/192.3 MB 372.6 kB/s eta 0:08:06\n",
      "   -- ------------------------------------ 11.5/192.3 MB 379.2 kB/s eta 0:07:57\n",
      "   -- ------------------------------------ 11.8/192.3 MB 385.9 kB/s eta 0:07:48\n",
      "   -- ------------------------------------ 12.3/192.3 MB 399.2 kB/s eta 0:07:31\n",
      "   -- ------------------------------------ 12.6/192.3 MB 405.9 kB/s eta 0:07:23\n",
      "   -- ------------------------------------ 12.8/192.3 MB 412.7 kB/s eta 0:07:15\n",
      "   -- ------------------------------------ 13.4/192.3 MB 426.1 kB/s eta 0:07:01\n",
      "   -- ------------------------------------ 13.6/192.3 MB 432.9 kB/s eta 0:06:53\n",
      "   -- ------------------------------------ 14.2/192.3 MB 446.3 kB/s eta 0:06:40\n",
      "   -- ------------------------------------ 14.4/192.3 MB 489.2 kB/s eta 0:06:04\n",
      "   --- ----------------------------------- 14.9/192.3 MB 503.8 kB/s eta 0:05:53\n",
      "   --- ----------------------------------- 15.5/192.3 MB 517.8 kB/s eta 0:05:42\n",
      "   --- ----------------------------------- 15.7/192.3 MB 525.2 kB/s eta 0:05:37\n",
      "   --- ----------------------------------- 16.3/192.3 MB 539.1 kB/s eta 0:05:27\n",
      "   --- ----------------------------------- 16.8/192.3 MB 553.4 kB/s eta 0:05:18\n",
      "   --- ----------------------------------- 17.3/192.3 MB 567.5 kB/s eta 0:05:09\n",
      "   --- ----------------------------------- 17.8/192.3 MB 581.4 kB/s eta 0:05:01\n",
      "   --- ----------------------------------- 18.4/192.3 MB 595.3 kB/s eta 0:04:53\n",
      "   --- ----------------------------------- 18.9/192.3 MB 608.8 kB/s eta 0:04:45\n",
      "   --- ----------------------------------- 19.7/192.3 MB 629.6 kB/s eta 0:04:35\n",
      "   ---- ---------------------------------- 20.2/192.3 MB 643.6 kB/s eta 0:04:28\n",
      "   ---- ---------------------------------- 21.0/192.3 MB 664.8 kB/s eta 0:04:18\n",
      "   ---- ---------------------------------- 21.8/192.3 MB 762.6 kB/s eta 0:03:44\n",
      "   ---- ---------------------------------- 22.5/192.3 MB 786.0 kB/s eta 0:03:37\n",
      "   ---- ---------------------------------- 23.6/192.3 MB 817.7 kB/s eta 0:03:27\n",
      "   ---- ---------------------------------- 24.6/192.3 MB 848.9 kB/s eta 0:03:18\n",
      "   ----- --------------------------------- 25.7/192.3 MB 881.0 kB/s eta 0:03:10\n",
      "   ----- --------------------------------- 27.0/192.3 MB 920.6 kB/s eta 0:03:00\n",
      "   ----- --------------------------------- 28.3/192.3 MB 959.5 kB/s eta 0:02:51\n",
      "   ------ -------------------------------- 29.6/192.3 MB 999.4 kB/s eta 0:02:43\n",
      "   ------ --------------------------------- 31.2/192.3 MB 1.0 MB/s eta 0:02:35\n",
      "   ------ --------------------------------- 32.8/192.3 MB 1.1 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 34.3/192.3 MB 1.1 MB/s eta 0:02:19\n",
      "   ------- -------------------------------- 36.4/192.3 MB 1.2 MB/s eta 0:02:10\n",
      "   ------- -------------------------------- 38.3/192.3 MB 1.3 MB/s eta 0:02:03\n",
      "   -------- ------------------------------- 40.6/192.3 MB 1.3 MB/s eta 0:01:55\n",
      "   -------- ------------------------------- 42.7/192.3 MB 1.4 MB/s eta 0:01:48\n",
      "   --------- ------------------------------ 45.4/192.3 MB 1.6 MB/s eta 0:01:35\n",
      "   --------- ------------------------------ 48.0/192.3 MB 1.6 MB/s eta 0:01:29\n",
      "   ---------- ----------------------------- 49.5/192.3 MB 1.7 MB/s eta 0:01:25\n",
      "   ---------- ----------------------------- 51.6/192.3 MB 1.7 MB/s eta 0:01:21\n",
      "   ----------- ---------------------------- 53.7/192.3 MB 1.8 MB/s eta 0:01:18\n",
      "   ----------- ---------------------------- 55.3/192.3 MB 1.8 MB/s eta 0:01:15\n",
      "   ----------- ---------------------------- 56.6/192.3 MB 1.9 MB/s eta 0:01:13\n",
      "   ------------ --------------------------- 58.2/192.3 MB 1.9 MB/s eta 0:01:11\n",
      "   ------------ --------------------------- 59.8/192.3 MB 2.0 MB/s eta 0:01:08\n",
      "   ------------ --------------------------- 61.3/192.3 MB 2.1 MB/s eta 0:01:04\n",
      "   ------------- -------------------------- 63.2/192.3 MB 2.1 MB/s eta 0:01:02\n",
      "   ------------- -------------------------- 64.7/192.3 MB 2.2 MB/s eta 0:01:00\n",
      "   ------------- -------------------------- 66.3/192.3 MB 2.2 MB/s eta 0:00:58\n",
      "   -------------- ------------------------- 68.2/192.3 MB 2.2 MB/s eta 0:00:56\n",
      "   -------------- ------------------------- 69.7/192.3 MB 2.3 MB/s eta 0:00:54\n",
      "   -------------- ------------------------- 71.6/192.3 MB 2.5 MB/s eta 0:00:49\n",
      "   --------------- ------------------------ 73.4/192.3 MB 2.6 MB/s eta 0:00:47\n",
      "   --------------- ------------------------ 75.0/192.3 MB 2.6 MB/s eta 0:00:46\n",
      "   --------------- ------------------------ 76.8/192.3 MB 2.6 MB/s eta 0:00:44\n",
      "   ---------------- ----------------------- 78.6/192.3 MB 2.7 MB/s eta 0:00:43\n",
      "   ---------------- ----------------------- 80.0/192.3 MB 2.7 MB/s eta 0:00:42\n",
      "   ----------------- ---------------------- 81.8/192.3 MB 2.8 MB/s eta 0:00:41\n",
      "   ----------------- ---------------------- 83.6/192.3 MB 2.8 MB/s eta 0:00:39\n",
      "   ----------------- ---------------------- 85.5/192.3 MB 2.8 MB/s eta 0:00:38\n",
      "   ------------------ --------------------- 87.3/192.3 MB 2.9 MB/s eta 0:00:37\n",
      "   ------------------ --------------------- 88.9/192.3 MB 2.9 MB/s eta 0:00:36\n",
      "   ------------------ --------------------- 90.7/192.3 MB 3.1 MB/s eta 0:00:33\n",
      "   ------------------- -------------------- 92.5/192.3 MB 3.1 MB/s eta 0:00:32\n",
      "   ------------------- -------------------- 94.4/192.3 MB 3.2 MB/s eta 0:00:31\n",
      "   ------------------- -------------------- 95.9/192.3 MB 3.2 MB/s eta 0:00:31\n",
      "   -------------------- ------------------- 98.0/192.3 MB 3.2 MB/s eta 0:00:30\n",
      "   -------------------- ------------------- 99.9/192.3 MB 3.3 MB/s eta 0:00:29\n",
      "   --------------------- ------------------ 101.7/192.3 MB 3.3 MB/s eta 0:00:28\n",
      "   --------------------- ------------------ 103.3/192.3 MB 3.4 MB/s eta 0:00:26\n",
      "   --------------------- ------------------ 105.4/192.3 MB 3.5 MB/s eta 0:00:26\n",
      "   ---------------------- ----------------- 107.2/192.3 MB 3.5 MB/s eta 0:00:25\n",
      "   ---------------------- ----------------- 109.3/192.3 MB 3.6 MB/s eta 0:00:24\n",
      "   ----------------------- ---------------- 111.1/192.3 MB 3.7 MB/s eta 0:00:22\n",
      "   ----------------------- ---------------- 113.2/192.3 MB 3.8 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 115.1/192.3 MB 3.8 MB/s eta 0:00:21\n",
      "   ------------------------ --------------- 117.2/192.3 MB 3.9 MB/s eta 0:00:20\n",
      "   ------------------------ --------------- 119.0/192.3 MB 3.9 MB/s eta 0:00:19\n",
      "   ------------------------- -------------- 121.1/192.3 MB 3.9 MB/s eta 0:00:19\n",
      "   ------------------------- -------------- 123.2/192.3 MB 4.1 MB/s eta 0:00:18\n",
      "   -------------------------- ------------- 125.0/192.3 MB 4.1 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 127.1/192.3 MB 4.2 MB/s eta 0:00:16\n",
      "   -------------------------- ------------- 129.2/192.3 MB 4.2 MB/s eta 0:00:15\n",
      "   --------------------------- ------------ 131.3/192.3 MB 4.3 MB/s eta 0:00:15\n",
      "   --------------------------- ------------ 133.4/192.3 MB 4.3 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 135.5/192.3 MB 4.4 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 136.8/192.3 MB 4.4 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 137.9/192.3 MB 4.5 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 138.7/192.3 MB 4.5 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 139.5/192.3 MB 4.6 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 140.2/192.3 MB 4.6 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 141.0/192.3 MB 4.6 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 141.8/192.3 MB 4.6 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 142.6/192.3 MB 4.6 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 143.4/192.3 MB 4.7 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 144.4/192.3 MB 4.7 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 145.2/192.3 MB 4.7 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 145.8/192.3 MB 4.7 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 146.5/192.3 MB 4.7 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 147.1/192.3 MB 4.7 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 147.8/192.3 MB 4.8 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 148.4/192.3 MB 4.8 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 149.2/192.3 MB 4.8 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 149.9/192.3 MB 4.8 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 150.5/192.3 MB 4.8 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 151.3/192.3 MB 4.8 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 152.0/192.3 MB 4.9 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 152.6/192.3 MB 4.9 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 153.4/192.3 MB 4.9 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 154.1/192.3 MB 4.9 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 154.9/192.3 MB 4.9 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 155.7/192.3 MB 4.9 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 156.5/192.3 MB 5.0 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 157.5/192.3 MB 5.0 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 158.3/192.3 MB 5.0 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 159.1/192.3 MB 5.2 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 159.9/192.3 MB 5.1 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 161.0/192.3 MB 5.1 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 161.7/192.3 MB 5.1 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 162.5/192.3 MB 5.2 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 163.6/192.3 MB 5.2 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 164.4/192.3 MB 5.2 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 165.4/192.3 MB 5.2 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 166.2/192.3 MB 5.4 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 167.2/192.3 MB 5.4 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 168.3/192.3 MB 5.4 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 169.3/192.3 MB 5.4 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 170.4/192.3 MB 5.4 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 171.4/192.3 MB 5.4 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 172.5/192.3 MB 5.5 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 173.5/192.3 MB 5.5 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 174.6/192.3 MB 5.5 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 175.9/192.3 MB 5.5 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 176.9/192.3 MB 5.6 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 178.0/192.3 MB 5.6 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 179.0/192.3 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 180.1/192.3 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 181.4/192.3 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 182.5/192.3 MB 5.7 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 183.8/192.3 MB 5.8 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 184.8/192.3 MB 5.8 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 186.1/192.3 MB 5.8 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 187.2/192.3 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  188.5/192.3 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  189.5/192.3 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  190.3/192.3 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  191.4/192.3 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  192.2/192.3 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  192.2/192.3 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 192.3/192.3 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading torchtext-0.16.0-cp311-cp311-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 4.7 MB/s eta 0:00:00\n",
      "Downloading torchdata-0.7.0-cp311-cp311-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 1.0/1.3 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 5.3 MB/s eta 0:00:00\n",
      "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-19.0.1-cp311-cp311-win_amd64.whl (25.3 MB)\n",
      "   ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/25.3 MB 5.6 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.1/25.3 MB 5.6 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 3.1/25.3 MB 5.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.5/25.3 MB 5.6 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 5.8/25.3 MB 5.6 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.8/25.3 MB 5.5 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 7.6/25.3 MB 5.5 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 8.9/25.3 MB 5.3 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 10.0/25.3 MB 5.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 11.3/25.3 MB 5.4 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 12.3/25.3 MB 5.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.6/25.3 MB 5.5 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.9/25.3 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 16.0/25.3 MB 5.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 17.3/25.3 MB 5.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.4/25.3 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.7/25.3 MB 5.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 21.0/25.3 MB 5.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.3/25.3 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.3/25.3 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.6/25.3 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.3/25.3 MB 5.5 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, pyarrow, fsspec, dill, torch, multiprocess, torchdata, torchtext, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.2.0\n",
      "    Uninstalling fsspec-2025.2.0:\n",
      "      Successfully uninstalled fsspec-2025.2.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0\n",
      "    Uninstalling torch-2.6.0:\n",
      "      Successfully uninstalled torch-2.6.0\n",
      "Successfully installed datasets-3.3.2 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 pyarrow-19.0.1 torch-2.1.0 torchdata-0.7.0 torchtext-0.16.0 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets torch==2.1.0 torchtext==0.16.0 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hasan\\Downloads\\Alanbrb\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\hasan\\Downloads\\Alanbrb\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hasan\\.cache\\huggingface\\hub\\datasets--facebook--natural_reasoning. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 1145824/1145824 [00:11<00:00, 103652.52 examples/s]\n",
      "Map: 100%|██████████| 1145824/1145824 [01:14<00:00, 15363.63 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input: Question: What is the total work done on an object when it is moved upwards against gravity, considering both the change in kinetic energy and potential energy? Use the Work-Energy Theorem and the principle of conservation of mechanical energy to derive your answer.\n",
      "Sample output: Answer: W = delta ME = delta KE + delta PE\n",
      "Vocab size: 10000\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"facebook/natural_reasoning\")\n",
    "\n",
    "# Format data: question -> reference_answer\n",
    "def format_data(example):\n",
    "    input_text = f\"Question: {example['question']}\"\n",
    "    output_text = f\"Answer: {example['reference_answer']}\"\n",
    "    return {\"input\": input_text, \"output\": output_text}\n",
    "\n",
    "train_data = dataset[\"train\"].map(format_data)\n",
    "\n",
    "# Tokenizer and vocabulary\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data):\n",
    "    for example in data:\n",
    "        yield tokenizer(example[\"input\"])\n",
    "        yield tokenizer(example[\"output\"])\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"], max_tokens=10000)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Convert text to tensors\n",
    "def text_to_tensor(text, vocab, max_len=100):\n",
    "    tokens = [\"<sos>\"] + tokenizer(text)[:max_len-2] + [\"<eos>\"]\n",
    "    return torch.tensor([vocab[token] for token in tokens], dtype=torch.long)\n",
    "\n",
    "train_inputs = [text_to_tensor(ex[\"input\"], vocab) for ex in train_data]\n",
    "train_outputs = [text_to_tensor(ex[\"output\"], vocab) for ex in train_data]\n",
    "\n",
    "# Verify\n",
    "print(\"Sample input:\", train_data[0][\"input\"])\n",
    "print(\"Sample output:\", train_data[0][\"output\"])\n",
    "print(\"Vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class ReasoningDataset(Dataset):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, outputs = zip(*batch)\n",
    "    inputs = pad_sequence(inputs, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    outputs = pad_sequence(outputs, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    return inputs, outputs\n",
    "\n",
    "dataset = ReasoningDataset(train_inputs, train_outputs)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 5232400\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder = nn.GRU(embed_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.decoder = nn.GRU(embed_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        vocab_size = self.fc.out_features\n",
    "\n",
    "        # Encode\n",
    "        embedded = self.embedding(src)\n",
    "        enc_output, hidden = self.encoder(embedded)\n",
    "\n",
    "        # Decode\n",
    "        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(src.device)\n",
    "        input = tgt[:, 0]  # <sos>\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            embedded = self.embedding(input).unsqueeze(1)\n",
    "            dec_output, hidden = self.decoder(embedded, hidden)\n",
    "            output = self.fc(dec_output.squeeze(1))\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input = tgt[:, t] if teacher_force else output.argmax(1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Initialize model\n",
    "model = Seq2SeqModel(vocab_size=len(vocab))\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 2078992\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128):  # Reduced from 128 and 256\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder = nn.GRU(embed_dim, hidden_dim, num_layers=1, batch_first=True)  # Reduced to 1 layer\n",
    "        self.decoder = nn.GRU(embed_dim, hidden_dim, num_layers=1, batch_first=True)  # Reduced to 1 layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        vocab_size = self.fc.out_features\n",
    "\n",
    "        # Encode\n",
    "        embedded = self.embedding(src)\n",
    "        enc_output, hidden = self.encoder(embedded)\n",
    "\n",
    "        # Decode\n",
    "        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(src.device)\n",
    "        input = tgt[:, 0]  # <sos>\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            embedded = self.embedding(input).unsqueeze(1)\n",
    "            dec_output, hidden = self.decoder(embedded, hidden)\n",
    "            output = self.fc(dec_output.squeeze(1))\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input = tgt[:, t] if teacher_force else output.argmax(1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Initialize model with vocab_size from your code\n",
    "vocab_size = len(vocab)  # 10,000 in your case\n",
    "model = Seq2SeqModel(vocab_size=vocab_size, embed_dim=64, hidden_dim=128)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      9\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hasan\\Downloads\\Alanbrb\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:626\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;49;00m\n\u001b[0;32m    629\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\hasan\\Downloads\\Alanbrb\\.venv\\Lib\\site-packages\\torch\\autograd\\profiler.py:648\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m--> 648\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[1;32mc:\\Users\\hasan\\Downloads\\Alanbrb\\.venv\\Lib\\site-packages\\torch\\_ops.py:448\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        output = output[:, 1:, :].reshape(-1, len(vocab))\n",
    "        tgt = tgt[:, 1:].reshape(-1)\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"reasoning_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, model, vocab, tokenizer, max_len=50):\n",
    "    model.eval()\n",
    "    input_text = f\"Question: {question}\"\n",
    "    src = text_to_tensor(input_text, vocab, max_len=100).unsqueeze(0).to(device)\n",
    "\n",
    "    output = [vocab[\"<sos>\"]]\n",
    "    hidden = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedded = model.embedding(src)\n",
    "        _, hidden = model.encoder(embedded)\n",
    "        input = torch.tensor([vocab[\"<sos>\"]], dtype=torch.long).to(device)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            embedded = model.embedding(input).unsqueeze(1)\n",
    "            dec_output, hidden = model.decoder(embedded, hidden)\n",
    "            pred = model.fc(dec_output.squeeze(1)).argmax(1).item()\n",
    "            output.append(pred)\n",
    "            if pred == vocab[\"<eos>\"]:\n",
    "                break\n",
    "            input = torch.tensor([pred], dtype=torch.long).to(device)\n",
    "\n",
    "    return \" \".join(vocab.lookup_tokens(output[1:-1]))\n",
    "\n",
    "# Test\n",
    "question = \"What is the total work done on an object when it is moved upwards against gravity, considering both the change in kinetic energy and potential energy? Use the Work-Energy Theorem and the principle of conservation of mechanical energy to derive your answer.\"\n",
    "print(\"Generated answer:\", generate_answer(question, model, vocab, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
